#### USAGE
#### 1) First load snakemake
#### module load NiaEnv/2018a
#### module load python/3.6.4-anaconda5.1.0
#### source activate snakemake
#### snakemake -s AN_WGS_script/run_wes.snk --cores 1 -j 50 --cluster "sbatch -N 1 -t 1:00:00 --ntasks 80
#### --output=logs/%x_%j.log" --config input=tcga_wxs_sample_list_fastq.tsv
#### outdir=$SCRATCH/AN_WGS/20220606_tcga_wxs exome_bed=$SCRATCH/AN_WGS/AN_WGS_script/WES_bedfiles/whole_exome_agilent_1.1_refseq_plus_3_boosters.targetIntervals_GRCh38_liftover.bed --ri

import pandas as pd
import os
import time

# initial settings

#TMP = "$SCRATCH/temp"

localrules: all, symlink_reffiles, generate_intervals

###
### LOAD SAMPLES
###

configfile: "AN_WGS_script/niagara_config.yaml"

EXOME_BED = config['EXOME_BED']

print("\n***INPUT FILE: " + config['input'] + "***\n")
INPUT = pd.read_csv(config['input'],names = ['Patient','Sex','n_vs_t','Sample','Lane','Fastq1','Fastq2'],header=0)
INPUT['Lane'] = INPUT.Lane.apply(str)
INPUT['Sample_Lane'] = INPUT.Sample + "_" + INPUT.Lane
SAMPLE =  INPUT['Sample'].unique()
PAT = INPUT.Patient.drop_duplicates()
NORMAL = INPUT[(INPUT.n_vs_t == 0)].Sample.drop_duplicates()
SAMPLE_LANE = INPUT.Sample_Lane
print(INPUT)

### PRINT OUTPUT DIRECTORY
OUTDIR = config['outdir']
print("***OUTPUT DIRECTORY: " + OUTDIR + "***")
print("Reference directory is: " + os.environ['REF_DIR'])
config["reference"]["directory"] = os.environ['REF_DIR'] + "/"

###
###	REFERENCE FILES
###
REF_fasta = config["reference"]["directory"] + config["reference"]["fasta"]
REF_dbsnp = config["reference"]["directory"] + config["reference"]["dbsnp"]
REF_known_indels = config["reference"]["directory"] + config["reference"]["known_indels"]
REF_gnomAD = config["reference"]["directory"] + config["reference"]["gnomAD"]
REF_pon = config["reference"]["directory"] + config["reference"]["pon"]
REF_exac_common = config["reference"]["directory"] + config["reference"]["exac_common"]
### Some settings for TITAN
CLUST = {1:[1], 2:[1,2], 3:[1,2,3], 4:[1,2,3,4], 5:[1,2,3,4,5], 6:[1,2,3,4,5,6], 7:[1,2,3,4,5,6,7], 8:[1,2,3,4,5,6,7,8], 9:[1,2,3,4,5,6,7,8,9], 10:[1,2,3,4,5,6,7,8,9,10]}
PLOIDY = {2:[2], 3:[2,3], 4:[2,3,4]}
CHRS = ['chr1','chr2','chr3','chr4','chr5','chr6','chr7','chr8','chr9','chr10','chr11','chr12','chr13','chr14','chr15','chr16','chr17','chr18','chr19','chr20','chr21','chr22','chrX','chrY','chrM']




###
### Final Results
###
rule all:
	input:
		###FASTQC + BWA Alignment
		expand(OUTDIR + "/QC/fastqc/{sample_lane}/fastqc_complete", sample_lane = SAMPLE_LANE),
		expand(OUTDIR + "/Recal/{sample}.recal.bam", sample = SAMPLE),
		### using zip https://endrebak.gitbooks.io/the-snakemake-book/chapters/expand/expand.html
		###BAMQC + samtools_stats
		#expand(OUTDIR + "/QC/{sample}/{sample}.samtools.stats.out",sample = SAMPLE),
		expand(OUTDIR + "/QC/bamqc/{sample}/qualimapReport.html",sample = SAMPLE),
		####
		####VARIANT CALLING OUTPUTS
		####
		expand(OUTDIR + "/results/haplotypecaller/individual/{patient_normal}/{patient_normal}.merged.gvcf.gz",patient_normal = NORMAL)
		# OUTDIR + "/results/haplotypecaller/merged/annotated/vep_posterior_merged_all_chromosomes.indel.snp.recalibrated_99.9.vcf.gz"
	threads: 1
	container:
		config["singularity"]["multiqc"]
	params:
		time = time.strftime('%Y%m%d'),
		input_csv = config['input']
	shell:
		"""
		multiqc {OUTDIR} -n {OUTDIR}/pipeline_info/reports/multiqc_{params.time}.html
		cp {params.input_csv} {OUTDIR}/Sample_{params.time}.csv
		"""

#		mv multiqc OUTDIR/pipeline_info/report/

###
### Step by step
###


### Symlink all reference files to make singularity easier
rule symlink_reffiles:
	input:
		config["reference"]["directory"] + config["reference"]["fasta"],
		config["reference"]["directory"] + config["reference"]["dbsnp"],
		config["reference"]["directory"] + config["reference"]["known_indels"],
		config["reference"]["directory"] + config["reference"]["gnomAD"],
		config["reference"]["directory"] + config["reference"]["pon"],
		config["reference"]["directory"] + config["reference"]["exac_common"],
		config["reference"]["directory"] + config["reference"]["ascat_acloci"],
		config["reference"]["directory"] + config["reference"]["ascat_acloci_gc"]
	output:
		OUTDIR + "/reference/" + config["reference"]["fasta"],
		OUTDIR + "/reference/" + config["reference"]["dbsnp"],
		OUTDIR + "/reference/" + config["reference"]["known_indels"],
		OUTDIR + "/reference/" + config["reference"]["gnomAD"],
		OUTDIR + "/reference/" + config["reference"]["pon"],
		OUTDIR + "/reference/" + config["reference"]["exac_common"],
		OUTDIR + "/reference/" + config["reference"]["ascat_acloci"],
		OUTDIR + "/reference/" + config["reference"]["ascat_acloci_gc"]
	threads: 1
	params:
		original_ref_dir = config["reference"]["directory"],
		target_ref_dir = OUTDIR + "/reference/"
	shell:
		"""
		ln -sf {params.original_ref_dir}* {params.target_ref_dir}
		"""

##SET NUMBERS FROM 0000 to 0099
NUMBERS = [str(a)+str(b)+str(c)+str(d) for a in range(0,1) for b in range(0,1) for c in range(0,1) for d in range(0,10)]

rule generate_intervals:
	input:
		ref_fasta = OUTDIR + "/reference/" + config["reference"]["fasta"],
		exome_bed = EXOME_BED
	output:
		expand(OUTDIR + "/pipeline_info/intervals/{num}-scattered.interval_list", num = NUMBERS)
	threads: 1
	resources:
		time = 60,
		mem_mb = 2500
	container:
		config["singularity"]["gatk"]
	shell:
		"""
		gatk --java-options "-Xmx2g" SplitIntervals \
		-R {input.ref_fasta} \
		-L {input.exome_bed} \
		--scatter-count 10 \
		-O {OUTDIR}/pipeline_info/intervals
		"""


###Align
include: "snakemake_scripts/wgs_align.snk"

###Markdup recal
include: "snakemake_scripts/wgs_markdup_recal.snk"

###Bam QC
include: "snakemake_scripts/wes_bam_qc.snk"

###
### VARIANT CALLING FOR EACH TUMOUR INDIVIDUALLY
###
rule haplotypecaller:
	input:
		normal = OUTDIR +"/Recal/{patient_normal}.recal.bam",
		interval = OUTDIR + "/pipeline_info/intervals/{num}-scattered.interval_list",
		ref_fasta = OUTDIR + "/reference/" + config["reference"]["fasta"]
		# ref_dbsnp = OUTDIR + "/reference/" + config["reference"]["dbsnp"]
	output:
		vcf = temp(OUTDIR + "/results/haplotypecaller/individual/{patient_normal}/unfiltered_{patient_normal}.{num}.vcf.gz")
		#index =  temp(OUTDIR + "/results/haplotypecaller/{patient_normal}/unfiltered_{patient_normal}.{num}.vcf.tbi")
	threads: 2
	resources:
		time = 1*60,
		mem_mb = 10500
	container:
		config["singularity"]["gatk"]
	shell:
		"""
		gatk --java-options "-Xmx8g" HaplotypeCaller \
		-I {input.normal}  \
		-L {input.interval} \
		-R {input.ref_fasta} \
		-ERC GVCF \
		-GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 \
		-O {output.vcf}
		"""

def concat_hc_vcf(wildcards):
	return expand(OUTDIR + "/results/haplotypecaller/individual/" + wildcards.patient_normal + "/unfiltered_" + wildcards.patient_normal + ".{num}.vcf.gz", num = NUMBERS)

rule merge_haplotypecaller_vcf:
	input:
		vcf = concat_hc_vcf
	output:
		OUTDIR + "/results/haplotypecaller/individual/{patient_normal}/{patient_normal}.merged.gvcf.gz"
	threads: 2
	resources:
		time = 15,
		mem_mb = 4500
	container:
		config["singularity"]["gatk"]
	params:
		input_list = lambda wildcards, input: ' -I '.join(input.vcf)
	shell:
		"""
		gatk --java-options "-Xmx8G"  \
		MergeVcfs \
		-I {params.input_list} \
		--OUTPUT {output}
		"""

### Mutect2 individual
include: "snakemake_scripts/wes_germline.snk"
