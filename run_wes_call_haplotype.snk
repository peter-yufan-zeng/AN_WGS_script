#### USAGE
#### 1) First load snakemake
#### module load NiaEnv/2018a
#### module load python/3.6.4-anaconda5.1.0
#### source activate snakemake
#### snakemake -s AN_WGS_script/run_wes.snk --cores 1 -j 50 --cluster "sbatch -N 1 -t 1:00:00 --ntasks 80
#### --output=logs/%x_%j.log" --config input=tcga_wxs_sample_list_fastq.tsv
#### outdir=$SCRATCH/AN_WGS/20220606_tcga_wxs exome_bed=$SCRATCH/AN_WGS/AN_WGS_script/WES_bedfiles/whole_exome_agilent_1.1_refseq_plus_3_boosters.targetIntervals_GRCh38_liftover.bed --ri

import pandas as pd
import os
import time

# initial settings

#TMP = "$SCRATCH/temp"

localrules: all, symlink_reffiles, generate_intervals

###
### LOAD SAMPLES
###

configfile: "AN_WGS_script/niagara_config.yaml"

EXOME_BED = config['EXOME_BED']

print("\n***INPUT FILE: " + config['input'] + "***\n")
INPUT = pd.read_csv(config['input'],names = ['Patient','Sex','n_vs_t','Sample','Lane','Fastq1','Fastq2'],header=0)
INPUT['Lane'] = INPUT.Lane.apply(str)
INPUT['Sample_Lane'] = INPUT.Sample + "_" + INPUT.Lane
SAMPLE =  INPUT['Sample'].unique()
PAT = INPUT.Patient.drop_duplicates()
NORMAL = INPUT[(INPUT.n_vs_t == 0)].Sample.drop_duplicates()
SAMPLE_LANE = INPUT.Sample_Lane
print(INPUT)

### PRINT OUTPUT DIRECTORY
OUTDIR = config['outdir']
print("***OUTPUT DIRECTORY: " + OUTDIR + "***")


###
###	REFERENCE FILES
###
REF_fasta = config["reference"]["directory"] + config["reference"]["fasta"]
REF_dbsnp = config["reference"]["directory"] + config["reference"]["dbsnp"]
REF_known_indels = config["reference"]["directory"] + config["reference"]["known_indels"]
REF_gnomAD = config["reference"]["directory"] + config["reference"]["gnomAD"]
REF_pon = config["reference"]["directory"] + config["reference"]["pon"]
REF_exac_common = config["reference"]["directory"] + config["reference"]["exac_common"]
### Some settings for TITAN
CLUST = {1:[1], 2:[1,2], 3:[1,2,3], 4:[1,2,3,4], 5:[1,2,3,4,5], 6:[1,2,3,4,5,6], 7:[1,2,3,4,5,6,7], 8:[1,2,3,4,5,6,7,8], 9:[1,2,3,4,5,6,7,8,9], 10:[1,2,3,4,5,6,7,8,9,10]}
PLOIDY = {2:[2], 3:[2,3], 4:[2,3,4]}
CHRS = ['chr1','chr2','chr3','chr4','chr5','chr6','chr7','chr8','chr9','chr10','chr11','chr12','chr13','chr14','chr15','chr16','chr17','chr18','chr19','chr20','chr21','chr22','chrX','chrY','chrM']




###
### Final Results
###
rule all:
	input:
		####
		####VARIANT CALLING OUTPUTS
		####
		expand(OUTDIR + "/results/haplotypecaller/individual/{patient_normal}/{patient_normal}.merged.gvcf.gz",patient_normal = NORMAL),
		OUTDIR + "/results/haplotypecaller/merged/annotated/vep_posterior_merged_all_chromosomes.indel.snp.recalibrated_99.9.vcf.gz"
	threads: 1
	container:
		config["singularity"]["multiqc"]
	params:
		time = time.strftime('%Y%m%d'),
		input_csv = config['input']
	shell:
		"""
		multiqc {OUTDIR} -n {OUTDIR}/pipeline_info/reports/multiqc_{params.time}.html
		cp {params.input_csv} {OUTDIR}/Sample_{params.time}.csv
		"""

#		mv multiqc OUTDIR/pipeline_info/report/

###
### Step by step
###


### Symlink all reference files to make singularity easier
rule symlink_reffiles:
	input:
		config["reference"]["directory"] + config["reference"]["fasta"],
		config["reference"]["directory"] + config["reference"]["dbsnp"],
		config["reference"]["directory"] + config["reference"]["known_indels"],
		config["reference"]["directory"] + config["reference"]["gnomAD"],
		config["reference"]["directory"] + config["reference"]["pon"],
		config["reference"]["directory"] + config["reference"]["exac_common"],
		config["reference"]["directory"] + config["reference"]["ascat_acloci"],
		config["reference"]["directory"] + config["reference"]["ascat_acloci_gc"]
	output:
		OUTDIR + "/reference/" + config["reference"]["fasta"],
		OUTDIR + "/reference/" + config["reference"]["dbsnp"],
		OUTDIR + "/reference/" + config["reference"]["known_indels"],
		OUTDIR + "/reference/" + config["reference"]["gnomAD"],
		OUTDIR + "/reference/" + config["reference"]["pon"],
		OUTDIR + "/reference/" + config["reference"]["exac_common"],
		OUTDIR + "/reference/" + config["reference"]["ascat_acloci"],
		OUTDIR + "/reference/" + config["reference"]["ascat_acloci_gc"]
	threads: 1
	params:
		original_ref_dir = config["reference"]["directory"],
		target_ref_dir = OUTDIR + "/reference/"
	shell:
		"""
		ln -sf {params.original_ref_dir}* {params.target_ref_dir}
		"""

##SET NUMBERS FROM 0000 to 0099
NUMBERS = [str(a)+str(b)+str(c)+str(d) for a in range(0,1) for b in range(0,1) for c in range(0,1) for d in range(0,10)]

rule generate_intervals:
	input:
		ref_fasta = OUTDIR + "/reference/" + config["reference"]["fasta"],
		exome_bed = EXOME_BED
	output:
		expand(OUTDIR + "/pipeline_info/intervals/{num}-scattered.interval_list", num = NUMBERS)
	threads: 1
	resources:
		time = 60,
		mem_mb = 2500
	container:
		config["singularity"]["gatk"]
	shell:
		"""
		gatk --java-options "-Xmx2g" SplitIntervals \
		-R {input.ref_fasta} \
		-L {input.exome_bed} \
		--scatter-count 10 \
		-O {OUTDIR}/pipeline_info/intervals
		"""


# ###Align
# include: "snakemake_scripts/wgs_align.snk"

# ###Markdup recal
# include: "snakemake_scripts/wgs_markdup_recal.snk"

# ###Bam QC
# include: "snakemake_scripts/wes_bam_qc.snk"



### Mutect2 individual
include: "snakemake_scripts/wes_germline.snk"

