#cluster-cancel: "scancel "
restart-times: "0"
#jobscript: "slurm-jobscript.sh"
cluster: "slurm-submit.py"
#cluster: "sbatch -t {resources.time} --nodes 1 -o logs/slurm/{rule}_{wildcards} -e logs/slurm/{rule}_{wildcards} "
#cluster-status: "slurm-status.py"
max-jobs-per-second: "100"
max-status-checks-per-second: "0.015"
local-cores: 1
latency-wait: "5"
use-conda: "False"
use-singularity: "True"
jobs: "100"
printshellcmds: "True"

# Example resource configuration
# default-resources:
#   - runtime=100
#   - mem_mb=6000
#   - disk_mb=1000000
# # set-threads: map rule names to threads
# set-threads:
#   - single_core_rule=1
#   - multi_core_rule=10
# # set-resources: map rule names to resources in general
# set-resources:
#   - high_memory_rule:mem_mb=12000
#   - long_running_rule:runtime=1200
