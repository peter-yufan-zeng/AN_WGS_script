Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	1	catAlleleCountFiles
	4	combineTitanAndIchorCNA
	1	copyOptSolution
	23	getAlleleCountsByChr
	23	getHETsites
	1	ichorCNA
	2	read_counter
	4	runTitanCNA
	1	selectSolution
	61

[Tue Feb  9 22:22:54 2021]
rule read_counter:
    input: results/tumor_sample_1.filtered.bam
    output: results/readDepth/tumor_sample_1.bin10000.wig
    log: logs/readDepth/tumor_sample_1.bin10000.log
    jobid: 36
    wildcards: samples=tumor_sample_1, binSize=10000
    resources: mem=4

[Tue Feb  9 22:22:55 2021]
Error in rule read_counter:
    jobid: 36
    output: results/readDepth/tumor_sample_1.bin10000.wig
    log: logs/readDepth/tumor_sample_1.bin10000.log (check log file(s) for error message)
    shell:
        singularity exec -B $SCRATCH/igenomes_ref,$SCRATCH/AN_WGS,$SCRATCH/AN_WGS/raw,$SCRATCH/singularity_images /gpfs/fs0/scratch/n/nicholsa/zyfniu/singularity_images/ichorcna_pz.simg readCounter results/tumor_sample_1.filtered.bam -c chr1,chr2,chr3,chr4,chr5,chr6,chr7,chr8,chr9,chr10,chr11,chr12,chr13,chr14,chr15,chr16,chr17,chr18,chr19,chr20,chr21,chr22,chrX,chrY -w 10000 -q 20 > results/readDepth/tumor_sample_1.bin10000.wig 2> logs/readDepth/tumor_sample_1.bin10000.log
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Removing output files of failed job read_counter since they might be corrupted:
results/readDepth/tumor_sample_1.bin10000.wig
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /gpfs/fs0/scratch/n/nicholsa/zyfniu/singularity_images/TitanCNA/scripts/snakemake/.snakemake/log/2021-02-09T222254.508400.snakemake.log
